# -*- coding: utf-8 -*-
"""ARMBench_Object Detection and Segmentation_Visulaization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GK69tRIBWW-2Wir5iKLEorCW3euIsRsi

# Perception Of Robotics Project
### Authors
+ Sooraj
+ Sai
+ Suman
+ Karteek

# This notebook shows the downloading of the ARMBench dataset and visualization to look at the annotations
"""

import os
import random
import matplotlib.pyplot as plt
import cv2
import numpy as np
import torch
import torch.utils.data
import torchvision
from PIL import Image
from pycocotools.coco import COCO

"""# Download and Unzip ARMBench Dataset"""

!wget https://armbench-dataset.s3.amazonaws.com/segmentation/armbench-segmentation-0.1.tar.gz
!tar -xzf "/content/armbench-segmentation-0.1.tar.gz"

print('Mix-tote-object Image Data' , len(os.listdir('/content/armbench-segmentation-0.1/mix-object-tote/images')))
print('Same-object-transfer Image Data' , len(os.listdir('/content/armbench-segmentation-0.1/same-object-transfer-set/images')))
print('Zoomed-out-tote Image Data' , len(os.listdir('/content/armbench-segmentation-0.1/zoomed-out-tote-transfer-set/images')))

"""# As per the research paper page 4, Authors clearly mentioned that We observe that applying model weights trained on `mixobject-tote` to the `zoomed-out-tote-transfer-set (mAP50 = 0.25)` and `same-object-transfer-set subsets (mAP50 = 0.11)`

# Custom Dataset with COCO format
"""

class ARMBench(torch.utils.data.Dataset):
  def __init__(self, root, annotation, transforms=None):
    self.root = root
    self.transforms = transforms
    self.coco = COCO(annotation)
    self.ids = list(sorted(self.coco.imgs.keys()))

  def __getitem__(self, index):
    coco = self.coco
    img_id = self.ids[index]
    ann_ids = coco.getAnnIds(imgIds=img_id)
    coco_annotation = coco.loadAnns(ann_ids)
    cat_ids = coco.getCatIds()
    path = coco.loadImgs(img_id)[0]['file_name']
    img = Image.open(os.path.join(self.root, path))
    mask = np.stack([np.where(coco.annToMask(ann)>0,1,0) for ann in coco_annotation])
    num_objs = len(coco_annotation)

    boxes = []
    labels = []
    areas = []
    for i in range(num_objs):
      xmin = coco_annotation[i]['bbox'][0]
      ymin = coco_annotation[i]['bbox'][1]
      xmax = xmin + coco_annotation[i]['bbox'][2]
      ymax = ymin + coco_annotation[i]['bbox'][3]
      boxes.append([xmin, ymin, xmax, ymax])
      labels.append(coco_annotation[i]['category_id'])
      areas.append(coco_annotation[i]['area'])
      masks = mask.transpose((1,2,0))
      boxes = torch.as_tensor(boxes, dtype=torch.float32)
      labels = torch.as_tensor(labels, dtype=torch.int64)
      img_id = torch.tensor([img_id])
      areas = torch.as_tensor(areas, dtype=torch.float32)

      iscrowd = torch.zeros((num_objs,), dtype=torch.int64)

      my_annotation = {}
      my_annotation["boxes"] = boxes
      my_annotation["labels"] = labels
      my_annotation["image_id"] = img_id
      my_annotation["area"] = areas
      my_annotation["iscrowd"] = iscrowd

      if self.transforms is not None:
        img = self.transforms(img)
        masks = self.transforms(masks)
        my_annotation["masks"] = masks
        return img, my_annotation

  def __len__(self):
    return len(self.ids)

"""1. In COCO format, the bounding box is given as [xmin, ymin, width, height]; however, Faster R-CNN in PyTorch expects the bounding box as [xmin, ymin, xmax, ymax].
2. The inputs for a PyTorch model must be in tensor format. I defined get_transform() as below.
"""

def get_transform():
  custom_transforms = []
  custom_transforms.append(torchvision.transforms.ToTensor())
  return torchvision.transforms.Compose(custom_transforms)

"""Create a train and test dataloader using our ARMBench dataset class"""

train_data_dir = '/content/armbench-segmentation-0.1/mix-object-tote/images'
train_armbench = '/content/armbench-segmentation-0.1/mix-object-tote/train.json'
test_armbench = '/content/armbench-segmentation-0.1/mix-object-tote/test.json'

train_dataset = ARMBench(root=train_data_dir, annotation=train_armbench, transforms=get_transform())
test_dataset = ARMBench(root=train_data_dir, annotation=test_armbench, transforms=get_transform())
def collate_fn(batch):
  return tuple(zip(*batch))
train_batch_size = 5
test_batch_size = 1
train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, collate_fn=collate_fn)
test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, collate_fn=collate_fn)

len(train_dataset), len(test_dataset)

print(len(train_dataloader),len(test_dataloader))

"""# Visualize dataset with annotations"""

images,labels = next(iter(train_dataloader))
plt.imshow(images[0].numpy().transpose(1,2,0))

def random_color_masks(image):
  colors = [[0, 255, 0],[0, 0, 255],[255, 0, 0],[0, 255, 255],[255, 255, 0],[255, 0, 255],[80, 70, 180], [250, 80, 190],[245, 145, 50],[70, 150, 250],[50, 190, 190]]
  r = np.zeros_like(image).astype(np.uint8)
  g = np.zeros_like(image).astype(np.uint8)
  b = np.zeros_like(image).astype(np.uint8)
  r[image==1], g[image==1], b[image==1] = colors[random.randrange(0, 10)]
  colored_mask = np.stack([r,g,b], axis=2)
  return colored_mask

def preprocess(img,pred,threshold=0.5):
  INSTANCE_CATEGORY_NAMES = ["Tote","Object"]
  masks = (pred[0]['masks'] ==True).squeeze().detach().cpu().numpy()
  pred_class = [INSTANCE_CATEGORY_NAMES[i-1] for i in list(pred[0]['labels'].cpu().numpy())]
  pred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in list(pred[0]['boxes'].detach().cpu().numpy())]
  return masks, pred_boxes, pred_class

def vis_segmentation(images,labels, threshold=0.9, rect_th=3,
                          text_size=3, text_th=3, url=False):
  masks, boxes, pred_cls = preprocess(images,labels,threshold=threshold)
  img = images[0].numpy().transpose(1,2,0)
  img = img*255
  img = img.astype(np.uint8)
  for i in range(len(masks)):
    rgb_mask = random_color_masks(masks[i])
    img = cv2.addWeighted(img, 1, rgb_mask, 0.5, 1)
    pt1 = tuple(int(x) for x in boxes[i][0])
    pt2 = tuple(int(x) for x in boxes[i][1])
    cv2.rectangle(img, pt1, pt2, color=(0, 255, 0), thickness=rect_th)
    cv2.putText(img, pred_cls[i], pt1, cv2.FONT_HERSHEY_SIMPLEX, text_size, (0, 255, 0), thickness=text_th)
  return img, pred_cls, masks[1]

out,cls,instances = vis_segmentation(images,labels)
plt.imshow(out)

